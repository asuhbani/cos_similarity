{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6jPo29FqDrlrMDHwv3p9F"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYg-4WnxIbgi",
        "outputId": "4c29f2fe-08f6-4780-c0fc-e31ec4214274"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEuvWWwiKVr9",
        "outputId": "c29fe936-6d75-4551-9657-38317f44dec1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from PyPDF2) (4.5.0)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tashaphyne"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKQPeSz8Y6TF",
        "outputId": "7111355d-c39d-4b4f-9d69-763be5be630b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tashaphyne\n",
            "  Downloading Tashaphyne-0.3.6-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.5/251.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from pyarabic->tashaphyne) (1.16.0)\n",
            "Installing collected packages: pyarabic, tashaphyne\n",
            "Successfully installed pyarabic-0.6.15 tashaphyne-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "import PyPDF2\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from tashaphyne.stemming import ArabicLightStemmer\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0EHVGyhKc0S",
        "outputId": "0edae8a0-f7fd-45ac-d524-4db2cd1345fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# المسار إلى المجلد الذي تحتوي عليه ملفات الإجابات (في هذا المثال D:/Answers)\n",
        "folder_path = '/content/drive/MyDrive/cos_sim'\n"
      ],
      "metadata": {
        "id": "gjCVeJYiW3NZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  ذكر اسم الملف مع نسبة التشابة\n",
        "\n",
        "# إعداد المعالج المناسب لتحليل النص\n",
        "stop_words = set(stopwords.words('english'))\n",
        "ps = PorterStemmer()\n",
        "als = ArabicLightStemmer()\n",
        "\n",
        "# قائمة الإجابات وأسماء الملفات\n",
        "answers_list = []\n",
        "file_names = []\n",
        "\n",
        "# قراءة جميع ملفات الإجابات في المجلد وإضافتها إلى قائمة الإجابات وأسماء الملفات\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.pdf'):\n",
        "        pdf_file = open(os.path.join(folder_path, filename), 'rb')\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        # استخراج النص الخام من الصفحات في الملف الحالي\n",
        "        text = ''\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "        # تجهيز النص لتحليله\n",
        "        if 'ar' in filename.lower():\n",
        "            # تحليل النص باللغة العربية\n",
        "            text = als.light_stem(text)\n",
        "        else:\n",
        "            # تحليل النص باللغة الإنجليزية\n",
        "            tokens = word_tokenize(text)\n",
        "            # إزالة الكلمات الوظيفية\n",
        "            filtered_tokens = [w for w in tokens if not w in stop_words]\n",
        "            # تطبيق الجذر الأساسي على الكلمات\n",
        "            stemmed_tokens = [ps.stem(w) for w in filtered_tokens]\n",
        "            text = ' '.join(stemmed_tokens)\n",
        "        # إضافة النص المعالج إلى قائمة الإجابات واسم الملف إلى قائمة أسماء الملفات\n",
        "        answers_list.append(text)\n",
        "        file_names.append(filename)\n",
        "\n",
        "# حساب درجات التشابه بين جميع الإجابات باستخدام مقياس التشابه Cosine Similarity\n",
        "similarities = []\n",
        "for i in range(len(answers_list)):\n",
        "    for j in range(i+1, len(answers_list)):\n",
        "        answer1 = answers_list[i]\n",
        "        answer2 = answers_list[j]\n",
        "        # تحويل النص إلى قطع من المصفوفات لحساب Cosine Similarity\n",
        "        answer1_tokens = answer1.split()\n",
        "        answer2_tokens = answer2.split()\n",
        "        all_tokens = set(answer1_tokens + answer2_tokens)\n",
        "        answer1_vector = [answer1_tokens.count(token) for token in all_tokens]\n",
        "        answer2_vector = [answer2_tokens.count(token) for token in all_tokens]\n",
        "        # حساب Cosine Similarity بين النصين\n",
        "        dot_product = sum([answer1_vector[k] * answer2_vector[k] for k in range(len(all_tokens))])\n",
        "        norm_answer1 = sum([x**2 for x in answer1_vector]) ** 0.5\n",
        "        norm_answer2 = sum([x**2 for x in answer2_vector]) ** 0.5\n",
        "        similarity = dot_product / (norm_answer1 * norm_answer2)\n",
        "        # إضافة النتيجة إلى قائمة النتائج\n",
        "        similarities.append((file_names[i] + ' and ' + file_names[j], similarity))\n",
        "\n",
        "# طباعة قائمة بأسماء الملفات ونسبة التشابه المحسوبة لكل منها\n",
        "for pair in similarities:\n",
        "    print(pair[0] + ' : ' + str(pair[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg0gK2bFYzjL",
        "outputId": "2bdffcd5-41b9-44e4-eff7-eda71907a5bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assignment_1.pdf and Assignment_2.pdf : 0.32292989923754106\n",
            "Assignment_1.pdf and Assignment_3.pdf : 0.32292989923754106\n",
            "Assignment_1.pdf and Assignment_4.pdf : 0.9999999999999998\n",
            "Assignment_1.pdf and Assignment_5.pdf : 0.9789922711782547\n",
            "Assignment_2.pdf and Assignment_3.pdf : 1.0\n",
            "Assignment_2.pdf and Assignment_4.pdf : 0.32292989923754106\n",
            "Assignment_2.pdf and Assignment_5.pdf : 0.3200072720569957\n",
            "Assignment_3.pdf and Assignment_4.pdf : 0.32292989923754106\n",
            "Assignment_3.pdf and Assignment_5.pdf : 0.3200072720569957\n",
            "Assignment_4.pdf and Assignment_5.pdf : 0.9789922711782547\n"
          ]
        }
      ]
    }
  ]
}